{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "# import built-in libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "\n",
    "# import functions from tslearn\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from tslearn.svm import TimeSeriesSVC\n",
    "\n",
    "# import functions from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that aggregates all csv files\n",
    "def aggregate_data(csv_dir, label):\n",
    "    \n",
    "    # get the list of all csv files\n",
    "    csv_lst = glob(os.path.join(csv_dir, '*.csv'))\n",
    "    \n",
    "    # compile a regular expression to extract\n",
    "    # the digits part of the file name\n",
    "    r = re.compile('\\D*(\\d*).csv')\n",
    "    \n",
    "    # create a lambda expression with this regular expression\n",
    "    extract_num = lambda x:int(r.search(x).group(1))\n",
    "    \n",
    "    # sort the file name in ascending order\n",
    "    csv_lst.sort(key=extract_num)\n",
    "                               \n",
    "    # create an empty dataframe to hold the results\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # process each csv file in turn\n",
    "    for csv in csv_lst:\n",
    "        \n",
    "        # read in the data from this csv file\n",
    "        df_tmp = pd.read_csv(csv)\n",
    "        \n",
    "        # strip the white spaces before and after the column names\n",
    "        df_tmp.columns = df_tmp.columns.str.strip()\n",
    "        \n",
    "        # add a column to denote the video id\n",
    "        df_tmp['vid'] = extract_num(csv)\n",
    "        \n",
    "        # add another column to denote the label\n",
    "        df_tmp['label'] = label\n",
    "        \n",
    "        # append this dataframe to the aggregated dataframe\n",
    "        df = df.append(df_tmp, ignore_index=True)\n",
    "        \n",
    "        # reset the index without keeping the old one\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # return the resulting dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that sanitizes data\n",
    "def sanitize_data(df):\n",
    "    \n",
    "    # copy the dataset to tmp\n",
    "    df_tmp = df.copy()\n",
    "    \n",
    "    # create a filter that filters out unsuccessful entries\n",
    "    filt1 = df_tmp['success'] != 1\n",
    "    \n",
    "    # remove unsuccessful entries\n",
    "    df_tmp.drop(index=df.loc[filt1].index, inplace=True)\n",
    "    \n",
    "    # create a filter that filters out low-confidence entries\n",
    "    filt2 = df_tmp['confidence'] < 0.8\n",
    "    \n",
    "    # remove low-confidence entries\n",
    "    df_tmp.drop(index=df_tmp.loc[filt2].index, inplace=True)\n",
    "    \n",
    "    # reset the index without keeping the old one\n",
    "    df_tmp.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # return the resulting dataframe\n",
    "    return df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the root folder of the dataset\n",
    "root = 'dataset'\n",
    "\n",
    "# set the folder that contains the dataset for happy smiles\n",
    "dir_happy = os.path.join(root,'happy_frames_openface')\n",
    "\n",
    "# set the folder that contains the dataset for nervous smiles\n",
    "dir_nervous = os.path.join(root,'happy_frames_openface')\n",
    "\n",
    "# aggregate the data for happy smiles\n",
    "# the label for happy smiles is 1\n",
    "df_happy = aggregate_data(dir_happy, 1)\n",
    "\n",
    "# aggregate the data for nervous smiles\n",
    "# the label for nervous smiles is 0\n",
    "df_nervous = aggregate_data(dir_nervous, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data sanitizations\n",
    "\n",
    "# the data for the happy smiles\n",
    "df_happy = sanitize_data(df_happy)\n",
    "\n",
    "# the data for the nervous smiles\n",
    "df_nervous = sanitize_data(df_nervous)\n",
    "\n",
    "# save to files\n",
    "df_happy.to_csv('happy_smiles.csv')\n",
    "df_nervous.to_csv('nervous_smiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that groups frames \n",
    "# belonging to each video\n",
    "def group_featrues(df_features):\n",
    "    \n",
    "    # create an empty list to hold the results\n",
    "    lst = []\n",
    "    \n",
    "    # process each entry by video id\n",
    "    for vid in df_features['vid'].unique():\n",
    "        \n",
    "        # create a filter that leave frames\n",
    "        # belonging to this vid alone\n",
    "        filt = df_features['vid'] == vid\n",
    "        \n",
    "        # extract the frames belonging to this vid\n",
    "        # convert to ndarray and add to the list\n",
    "        lst.append(df_features[filt].drop(['vid'],axis=1).to_numpy())\n",
    "    \n",
    "    # return the results as a ndarray\n",
    "    return np.array(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "\n",
    "# selected features for classification\n",
    "features = ['pose_Tx', 'pose_Ty', 'pose_Tz', 'AU06_r', 'AU12_r', 'AU26_r','vid']\n",
    "\n",
    "# extract features for happy smiles\n",
    "df_happy_features = df_happy.loc[:,features]\n",
    "\n",
    "# extract features for nervous smiles\n",
    "df_nervous_features = df_nervous.loc[:,features]\n",
    "\n",
    "# group features to a ndarray\n",
    "np_happy_features = group_featrues(df_happy_features)\n",
    "np_nervous_features = group_featrues(df_nervous_features)\n",
    "\n",
    "# create a combined features\n",
    "X = np.concatenate([np_happy_features,np_nervous_features])\n",
    "\n",
    "# create a combined labels\n",
    "y = np.concatenate([np.ones(np_happy_features.shape[0]), np.zeros(np_nervous_features.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into the training set and test set\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, shuffle=True, random_state=16, test_size = 0.2)\n",
    "\n",
    "# convert these features to time series\n",
    "\n",
    "# features from the training set \n",
    "X_train = to_time_series_dataset(X_train)\n",
    "\n",
    "# features from the test set\n",
    "X_test = to_time_series_dataset(X_test)\n",
    "\n",
    "# feature scaling\n",
    "\n",
    "# features from the training set \n",
    "#X_train = TimeSeriesScalerMinMax().fit_transform(X_train)\n",
    "\n",
    "# features from the test set\n",
    "#X_test = TimeSeriesScalerMinMax().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal kernel for the kernel SVM is gak\n",
      "The optimal C for the kernel SVM is 1\n"
     ]
    }
   ],
   "source": [
    "# model selection\n",
    "\n",
    "# create a kNN model with the metric of DTW \n",
    "svc = TimeSeriesSVC()\n",
    "\n",
    "# possible k values\n",
    "pgrid = {'kernel': ['gak', 'poly', 'rbf'], 'C':list(range(1,5))}\n",
    "\n",
    "# 5-fold cross-validation for training\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "# create a searcher for the optimal value of k\n",
    "grid_search = GridSearchCV(estimator=svc, n_jobs=6, param_grid=pgrid, cv=cv)\n",
    "\n",
    "# SVM parameter tuning with the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# get the optimal kernel from the grid search\n",
    "k = grid_search.best_params_['kernel']\n",
    "\n",
    "# get the optimal kernel\n",
    "print(f'The optimal kernel for the kernel SVM is {k}')\n",
    "\n",
    "# get the optimal C, penalty parameter C of the error term\n",
    "# from the grid search\n",
    "c = grid_search.best_params_['C']\n",
    "\n",
    "# get the optimal kernel\n",
    "print(f'The optimal C for the kernel SVM is {c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
